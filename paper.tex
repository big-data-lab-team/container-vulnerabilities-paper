% GigaScience template
\documentclass[a4paper,num-refs]{oup-contemporary}

\journal{gigascience}


%%%% Packages %%%%
\usepackage{siunitx}
\usepackage{algpseudocode} % Algorithmic environment
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{makecell}
\usepackage[flushleft]{threeparttable}

%%%% Commands %%%%
\newcommand{\todo}[1]{\color{red}\textbf{TODO:}#1\color{black}}
\newcommand{\note}[2]{\color{blue}Note: #1\color{black}}
\newcommand{\reprozip}[0]{ReproZip}
\newcommand{\TG}[1]{\color{blue}From Tristan: #1 \color{black}}

\title{Risks of Running Vulnerable Container Images for Data Analysis on HPC
Clusters}
  
\begin{document}

\author[1]{Bhupinder Kaur}
\author[1]{Aiman Hanna}
\author[1]{Tristan Glatard}

\affil[1]{Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada}

\maketitle

\begin{keywords}
Containers; Security; Docker; Singularity; Neuroimaging.
\end{keywords}


\section{Introduction}

Virtualization, which was introduced in 1960s, refers to the creation of a virtual
version of something. Its meaning has evolved and took different forms since then, for
instance hardware virtualization, containerization, desktop virtualization, etc.
Here, in this paper our main focus is on containerarization and its security concerns,
specifically in neuroimaging field.
Containers, which provide operating system (OS) level virtualization, are very
popular. This is due to the need for fast development cycles, continuous delivery,
and cost reduction of infrastructure. Moreover, demand for an isolated
environment adds on to the popularity of containers. Indeed, containers accelerate
development cycles as compared to virtual machines and provide near-native performance.
They are very lightweight, flexible, and more resource-efficient than virtual
machines. By providing OS level virtualization they
removes the overhead of having an extra OS layer.

Although Linux Containers (LXC) and other alternatives such as Rocket were already
available, container-based virtualization gained a lot of interest when Docker was
introduced in 2013~\cite{gantikow2016providing}. Docker makes container's use more straightforward. However, there
are few limitations of Docker. For example, it is not supported by shared High Performance
Computing (HPC) environments because its default configuration runs containers
as root. Secondly, Docker containers cannot be used for reproducibility as Docker
images can be updated, and hence losing specific package versions required for
reproducing scientific results. Consequently, Singularity was introduced in 2016 with the main
goal of removing drawbacks of Docker. Singularity was developed mainly to
support HPC environments and to provide reproducibility. \TG{The previous two paragraphs are too general. You should
summarize them in 2-3 sharper sentences.}

No doubt containers have gained a lot of popularity after Docker, however containers tightly
integrate with the host due to the sharing of the kernel, 
mounted file systems, and devices, which raises security
concerns. Figure \ref{fig:container-overview} provides overview of the containers architecture.
They also share three main security threats with other virtualization
techniques~\cite{gantikow2016providing}.

\begin{figure}
  \centering
  \includegraphics[width=.7\columnwidth]{Figures/container.png}
  \caption{Architecture of Container-based
                Virtualization.}
  \label{fig:container-overview}
\end{figure}

\textit{Privilege Escalation} A malicious attacker can break out of the container
and gain control of the host system and other containers running on the same host.

\textit{Denial-of-Service} One container may end up eating all the resources of the
host hence resulting in the starvation of the host and other containers.

\textit{Information Leak} Private data of the host and other containers could be
leaked and can be used for further attacks.


However, two features of the Linux kernel, \textit{control groups} and \textit{namespaces},
are used by containers to mitigate these threats to a certain extent. Security
of containers is still most concerning issue. According to a Forrester survey~\cite{} done in January
2015, security is the
biggest concern while deploying containers.
Except web applications, containers are also popularly used for scientific computing
and data science in neuroscience. In fact, Singularity was developed for scientific applications
by keeping focus on HPC environments. This popularity containers lead us to the
primary research question of this work: \textit{what is the current security state of
container images that are deployed on HPC clusters}.

To answer this question, we take a real use case of containers into consideration.
The goal of this paper is to report all vulnerabilities that are present in
container images, specifically used by neuroimaging field, and access their
related risks for data analysis. Particularly, we aim to answer three primary
research questions:

\textit{RQ1} Are vulnerabilities present inside container images that are
deployed on HPC clusters?
\textit{RQ2} Can we get rid of these vulnerabilities by updating the
images?
\textit{RQ3} Can we get rid of these vulnerabilities by shrinking the
size of the image?

To answer those questions, we take an example of neuroscience field where
containers are used on HPC clusters.

The existing work on containers focuses mainly on the security of Docker
containers.
This focus is justified by the fact that, containers expose the host's resources
(e.g., file system/ IPC) to the guest system. This feature raises a confidentiality
threat for the applications running on the same host. Previous studies evaluated
security of Docker engine ~\cite{martin2018docker, sultan2019container, combe2016docker, bui2015analysis},
and have scanned vulnerabilities on Docker hub~\cite{Shu2017, gummaraju2015over}.
Also Singularity came up with Stools, which can be used for scanning Singularity images
for security and quality checks.
Here, we focus on the specific context of scientific data analysis on HPC clusters, taking
neuroimaging as an example.

\TG{Some notions that should be introduced in this section (incomplete):
vulnerability vs malware, paper suggested by Emad Shihab.}

This paper is organized as follows: in next section, we provide description
of materials and methods which includes summarizing a particular use case of
containers in neuroscience, which is used for scientific computing. It involves
usage of containers on shared HPC cluster. Additionally, in this section we
describe all the resources that are used by this use case. We also explain
scanning tools that we use for scanning container images to report
vulnerabilities. Next section of this paper is about the results. Here, we
present number and type of vulnerabilites that are present in the container images
which are used for data analysis on shared HPC clusters, and discuss how many of them are
critical vulnerabilities. Following section is about discussion of these results
where we provide more insight into the risks that are caused by these vulnerabilties
and how they can be leveraged to design attacks. Here, we also provide some such
vulnerability examples. Finally, we conclude this paper by discussing ten simple
rules that can be followed to secure container image for use in clusters.

\section{Materials and Methods}

In this section we summarize a particular use case of containers, Canadian
Open Neuroscience Platform (CONP) \TG{No. In this section you describe the data and tools 
that were used in your experiments. The fact that they are used in CONP 
is a side note}. It is a national platform for sharing neuroscience
research data and aims at creating an interactive interface for neuroscientists and
clinical neuroscience. The main goal of the CONP platform is to store, process, and
distribute massive amounts of data produced by modern neuroscience.
Here, we summarize all the resources that are
used by CONP to function.

\subsection{Compute Canada Clusters}

Compute Canada is a national High Performance Computing (HPC) research platform.
It provides infrastructure and services for Advanced Research Computing (ARC).
Its goal is to bring excellence, ease, and innovation in research of Canadian
research institutions by providing storage, computing power, and required
softwares to carry out research.
It integrates with four regional partners namely ACENET, Calcul Qu√©bec, Compute
Ontario, and WestGrid for providing local support.
Compute Canada uses Slurm Workload Manager to schedule jobs and Lustre shared
file system to provide high bandwidth to its users.
Control groups are used by it to limit number of resources that can be used
by a particular user.
Compute Canada do not support Docker as its default configuration
runs containers as root. However, Compute Canada supports Singularity
containers because it removes the drawback of Docker.
\TG{Not sure if Compute Canada is used in the experiments. If not, remove and 
pitch it in the introduction that containers are nowadays used in many clusters, including 
Compute Canada. You could also refer to the full list of clusters that support singularity.}

\subsection{Container Engine: Singularity}

Singularity, which is introduced in 2016, provides operating system (OS) level
virtualization.
It offers mobility of compute by facilitating portable environments 
through a single image file~\cite{kurtzer2016singularity}. Once Singularity image
is created, it is hashed by using SHA256 hashing. Hence, it cannot be changed
once it is created, consequently, it can be used for reproducing the results of
scientific experiments. Singularity was introduced with the main goal of
providing support for multi-user environment. Main features provided by
Singularity are mobility of compute and reproducibility~\cite{kurtzer2017singularity}.
Mobility of compute is defined as creating and maintaining a workflow on
a local machine that can be easily ported on other hosts, Linux distributions,
and/or cloud service providers. To achieve this, Singularity packs everything
that is required for the application to run in a distributable image. Then that
image can be copied, archived, and shared. Moreover, Singularity containers
are easily portable across different versions of the C library and different kernel
implementations.
Same features of Singularity, which are used for mobility of compute,
facilitate reproducibility as well~\cite{sochat2017enhancing}.

\TG{This paragraph is too general for a method section. You might summarize this in
2-3 sentences for the introduction. Here you should talk about the engine,
i.e., how images are run, focusing on security features. Use of namespaces,
cgroups, access to devices, etc}

\TG{You should add a subsection presenting Docker.}

\subsection{Container Image Formats}

A container image is a package that contains the application along with its
required dependencies and libraries.
Nowadays, two most popular containerization tools used are Docker and Singularity.
Both have different image format. Below we provide description of both image
formats.

\subsubsection{Docker Image Format}

Docker images are organized as a series of layers stacked on top of each
other and identified by unique layer identifiers. If a layer is reused
in different images it will
have the same id across images. Each layer consists of a filesystem diff
with respect to the layer below. Layers are compressed \TG{Are you
referring to file compression similar to gzip?} into a single image and can
be pushed to DockerHub. There are two ways to create a Docker image. The
first is to use an existing Docker image as a base image and then build a
new image from that one, by making changes in the filesystem, which is
saved as a new layer of the Docker image. The second way is to build a new
Docker image from scratch. Docker supports several different storage drivers.
\textit{Overlay2} is the preferred storage driver used by Docker for all currently
supported Linux versions, whereas \textit{aufs} is preferred for Docker 18.06 and
older, when using Ubuntu 14.04 on kernel 3.13 as it do not support \textit{overlay2}.
Other storage drivers that are supported by Docker include \textit{devicemapper, btrfs,
zfs, and vfs}

\subsubsection{Singularity Image Format}

Singularity takes snapshot, locks, and archives the developed application
\TG{what does it mean?}, which is known as Singularity image. Due to image
hashing, Singularity does not contain image layers, unlike Docker images
\TG{is it really due to hashing? why couldn't a hashed image have layers?}.
So Singularity image is a single immutable file that can be moved around as
any other file \TG{Talking about the file system used would be relevant}.
While publishing experimental results, authors can also publish Singularity
image along with its hash, which allow other researchers to verify results.

\subsection{Application frameworks}

We used containerized applications available in the Boutiques and BIDS Apps
frameworks used in neuroscience to run analyses on HPC clusters. 

\subsubsection{Boutiques}

Boutiques~\cite{glatard2018boutiques} is a application sharing system to
publish, integrate, and execute command-line applications across different
platforms. In Boutiques, the command-line of the application is described
through a flexible JSON descriptor pointing to a Docker or a Singularity
image where the application is installed. A set of core tools help
construct, validate, publish and execute Boutiques descriptors. Boutiques
applications are published on the Zenodo research repository.

We used the 49 Boutiques applications published on Zenodo at the time of
the writing. These applications use 23 different container images,
including 7 Singularity images and 16 Docker images. \TG{You should add a
table showing the images available in each framework, with their base
image, DOIs, size, and number of files.}

\subsubsection{Brain Imaging Data Structure (BIDS) Apps}

BIDS apps~\cite{gorgolewski2017bids} is a framework designed to share and execute neuroimaging
analysis pipelines. It improves usability, accesibility, and reproducibility
of different applications through containerization.
\TG{How many BIDS apps are there? Did you use them all?} Currently, there are 27 different container images that are used by
BIDS App. All these are Docker images.

\subsubsection{ReproNim}

ReproNim is a Center for Reproducible Neuroimaging Computation. It is a vision
to help neuroscientists in reproducing neuroimaging research. Repronim
uses RepronIn, BrainVerse, Neuroimaging Computation Environments Manager (NICEMAN),
and NeuroBlast software tools for its
functioning. ReproIn is a software that automate acquisition and conversion
of collected MRI data to BIDS standard format with DataLad version management~\cite{kennedy2019everything}.
BrainVerse is a cross-platform software framework and collaborative desktop application
that helps in managing, tracking, and sharing information.
NICEMAN is a software system that tracks and manages available computation resources
and makes their use in a scalable and reproducible way.
Neuroblast is a service that facilitates data sharing of existing studies, and it also
helps users to search similar studies based on combination of analysis,
tasks and activation patterns. There are currently 28 different Singularity
images used by Repronim. 

\subsection{Other images}

\TG{We could also use selected images from DockerHub or SingularityHub. For
instances the ones used by AFNI or other neuroimaging tools not represented
in Boutiques or BIDS Apps. However we wouldn't be able to shrink these
images unless we identified specific command-lines to run.}

\subsection{Image Update Process}

If there exist vulnerabilities in the container image, then there are two ways
to proceed, either not to run that image at all, or run only after updating the
image. Updating the image involves updating all software packages that are
inside that image. However, this option effects the reproducibility
of the image, which means that the image cannot be used to verify research
findings of other scientists. An alternative approach can be to trim unnecessary
packages from images, which in turn reduces the number of vulnerabilities
present in images. \TG{Here you should describe the method you used to update the images, linking to the 
software (script) you used to do that. 
This paragraph is too general and doesn't belong to the methods. }


\subsection{Scanning tools used}

Out of the various container image scanning tools, we selected Anchore,
Vuls, Stools and ClairScanner for our experiments. \TG{Explain why you used
more than 1 tool, and why you used these ones speifically.}

\subsubsection{Anchore}

Anchore is an end-to-end, open-source container security platform. Anchore
is a static \TG{You should explain static vs dynamic scanning in the intro}
scanning tool that analyses container images and lists operating system
packages, unofficial packages, configuration files \TG{How about files
which are not configuration files?}, and language modules [58] \TG{Import
reference}. \TG{What is a language module?} \TG{Does it list unknown
packages, files or modules?} Anchore is a set of tools, which users can
employ for scanning container images and applying custom policies to them.
It provides visibility and transparency of the container environment.
Anchore offers services in two ways: Anchore Engine and Anchore Engine
Command Line Interface (CLI). Anchore Engine is available as a Docker
image, which is available on Docker Hub. Anchore Engine CLI provides
command line interface in addition to Anchore Engine REST API. Anchore
Engine CLI can be installed directly on the host and used to inspect
images.

\TG{You should refer to the vulnerability databases used, and explain how
software components, for instance files, are matched to these databases. Is
the matching based on file checksum?}

\subsubsection{Vuls}

Vuls is an open-source vulnerability scanner for Linux and FreeBSD, written in Go
language \TG{Why is it relevant that it's written in Go?}.
 It is an agentless vulnerability scanner and uses various
vulnerability databases \TG{You should list them and explain how they differ to 
the ones used by Anchore}. Using dynamic scanning technique, it offers
remote and local scan. Remote scan involves setting up only one machine, to
which other target servers are connected via SSH. Local scan involves running
the scan on the target server itself. Further, Vuls offer fast scan, fast root scan,
and deep scan.
Fast scan is performed without root privileges, whereas fast root scan is
performed with root privileges. The latter also uses checkrestart utility to detect processes that are updated but not yet restarted because if they are not
restarted, updates are not effective. So fast root scan also detects these pro-
cesses. Both fast scan and fast root scan are performed offline without Internet
access, and put almost no load on the scan target server. Deep scan uses
root privileges for scanning, and is used to get a more detailed scan. It checks
changelogs, which puts more load on the target server. As shown in Figure 4.3 \TG{There is no figure 4.3 or graph 4.5 here. Anyway it's not relevant 
to report results in this section. This paragraph looks like a copy paste from your previous report, it needs to be edited.}
it is clear that vulnerability detection time of Vuls is much better than An-
chore. Vuls tool uses dynamic scanning, so the scanning time does not depend
on the image size.
Graph 4.5 shows percentage of vulnerabilities that are
covered by Vuls and Anchore tools. This graph can be explained with the help
of List 4.4, which shows vulnerability databases that are referred, by both Vuls
and Anchore. Anchore covers less percentage of vulnerabilities as compared
to Vuls because Anchore relies mainly on vulnerability databases published by
Linux distributions, whereas Vuls additionally refers to another sources such as
databases from Japan, US-CERT, etc.

\subsubsection{Stools}

Stools combines CoreOS's ClairScanner with the Continuous Integration (Travis and
Circle) for scanning Singularity images for vulnerabilities.
Initially, ClairScanner was intented to scan Docker Containers. This tool does not fit well
to scan Singularity images because of the different image formats. Docker layers
consist of layers, whereas Singluarity images consist of a single binary file.
However, TravisCI and CircleCI are commonly used for testing code, and these
services also offer running containers. So Stools combined both the concepts.
It involves spinning a ClairScanner container during testing and building a
Singularity image, and scanning it for filesystem before the image is finalized.
\TG{Add link and/or reference.}

\subsection{CBRAIN}

\href{http://github.com/aces/cbrain}{CBRAIN}~\cite{sherif2014cbrain} is a web portal that is
used for the processing of distributed data on various storage locations of computing
clusters. CBRAIN system deployed by Montreal Neurological Institute depends on the infrastructure
delivered by Compute Canada~\cite{das2016mni}.
It is also used by CONP to exploit services of Compute Canada. CBRAIN is a collaborative
neuroimaging research platform that is active in production since 2009. CBRAIN
provides transparent access to various distributed storage sites and computing
resources across the world. CBRAIN‚Äôs infrastructure consists of three main
layers, namely access layer, service layer, and infrastructure layer~\cite{sherif2014cbrain}. The access
layer is for CBRAIN users, and can be accessed through any modern browser or
RESTful API. The service layer consists of CBRAIN portal, which contains
metadata and databases. It provides information about users, their permissions,
resources etc. The infrastructure layer consists of data resources and computing
resources, which are connected through a network. 
CBRAIN supports Docker, Singularity, BIDS apps, and Boutiques.
CBRAIN users uses a portal
account on the cluster. Users are only able to
run pre-defined applications on the cluster through the CBRAIN portal.

%\bibliographystyle{vancouver-authoryear}
\bibliography{bibliography}


\end{document}

