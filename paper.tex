% GigaScience template
\documentclass[a4paper,num-refs]{oup-contemporary}

\journal{gigascience}


%%%% Packages %%%%
\usepackage{siunitx}
\usepackage{minted} % Used for JSON highlighting
\usepackage{algpseudocode} % Algorithmic environment
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{makecell}
\usepackage[flushleft]{threeparttable}

%%%% Commands %%%%
\newcommand{\todo}[1]{\color{red}\textbf{TODO:}#1\color{black}}
\newcommand{\note}[2]{\color{blue}Note: #1\color{black}}
\newcommand{\reprozip}[0]{ReproZip}


\title{Risks of Running Vulnerable Container Images for Data Analysis on HPC
Clusters}
  
\begin{document}

\author[1]{Aiman Latif Hanna}
\author[1]{Bhupinder Kaur}
\author[1]{Tristan Glatard}

\affil[1]{Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada}

\maketitle


\section{Materials and Methods}

In this section we summarize a particular use case of containers which is Canadian
Open Neuroscience Platform (CONP). It is a national platform for sharing neuroscience
research data and aims at creating an interactive interface for neuroscientists and
clinical neuroscience. The main goal of the CONP platform is to store, process, and
distribute massive amounts of data produced by modern neuroscience.
Here, we summarize all the resources and software tools that are
used by CONP to function.

\subsection{Compute Canada Clusters}

Compute Canada is a national High Performance Computing (HPC) research platform.
It provides infrastructure and services for Advanced Research Computing (ARC).
Its goal is to bring excellence, ease, and innovation in research of Canadian
research institutions by providing storage, computing power, and required
softwares to carry out research.
It integrates with four regional partners namely ACENET, Calcul Québec, Compute
Ontario, and WestGrid for providing local support.
Compute Canada uses Slurm Workload Manager to schedule jobs.
Control groups are used to limit number of resources that can be used
by a particular user.
Compute Canada do not support Docker as its default configuration
runs containers as root. Whereas, Compute Canada supports Singularity
because it removes the drawback of Docker.
Compute Canada uses Lustre shared file system to provide high
bandwidth to its users.

\subsection{Container Engine: Singularity}

Singlarity, which came in 2016, provides operating system (OS)level.
It offers mobility of compute by facilitating portable environments 
through a single image file. Once Singularity image
is created, it is hashed by using SHA256 hashing. Hence, it cannot be changed
once it is created, consequently, it can be used for reproducing the results of
scientific experiments. Singularity was introduced with the main goal of pro-
viding support for multi-user environment. Main features provided by
Singularity are mobility of compute and reproducibility.
Mobility of compute is defined as creating and maintaining a workflow on
a local machine that can be easily ported on other hosts, Linux distributions,
and/or cloud service providers. To achieve this, Singularity packs everything
that is required for the application to run in a distributable image. Then that
image can be copied, archived, and shared. Moreover, Singularity containers
are easily portable across different versions of the C library and different kernel
implementations [16].
Same features of Singularity, which are used for mobility of compute, facil-
itate reproducibility as well.

\subsection{Container Image Formats}

A container image is a package that contains the application alongwith its
required dependencies and libraries.
Nowadays, two most popular containerization tools used are Docker and Singularity.
Both have different image format.

\subsubsection{Docker Image Format}

There are two ways to create Docker image. The first is to use an existing
Docker image as a base image and then build a new image from that one, by
making changes in the filesystem, which is saved as a new layer of the
Docker image. The second way is to build a new Docker image from scratch.
Docker images are organized as a series of layers which are stacked on top
of each other. Each layer consists of a filesystem diff that is introduced
due to the changes made on the layer below it. All layers of the Docker image
are identified by unique layerIDs. All these layers stacked together gives a
unified and complete view of a Docker image. These layers are
compressed into a single image and can be pushed to the Docker Hub.

\subsubsection{Singularity Image Format}

Singularity takes snapshot, locks, and archives the developed application, which
is known as Singularity image. Due to image hashing, Singularity does not
contain image layers, unlike Docker images. While publishing experimental
results, authors can also publish Singularity image along with its hash, which
allow other researchers to verify results.

\subsection{Application frameworks}

Following are the application frameworks in neuroscience that are using containers
on HPC clusters.

\subsubsection{Boutiques}

Boutiques is a application sharing system. It is based on Linux containers
and can be used to publish, integrate, and execute third-party
applications across different platforms automatically. Boutiques framework is
employed by CONP. With Boutiques, the command-line of the application is
described through a flexible template, which is called descriptor. Descriptor
describes the input that is required by the application and the output it produces.
It points to a Docker or Singularity image, where the application is installed.
Developers of the application produce Boutiques descriptor, and stores it along
with the application. Boutiques descriptor are described in JSON format. There
is a set of core tools which help in constructing, validating and executing bou-
tiques descriptors. During the application run-time, the execution platform
builds a command line by using input values given by the user and boutiques
descriptor of the application.

\subsubsection{Brain Imaging Data Structure (BIDS) Apps}

BIDS Apps is a framework designed to share and execute neuroimaging
analysis pipelines. It improves usability, accesibility, and reproducibility
of different operating system users as well as HPC users.

\subsubsection{ReproNim}

ReproNim: A Center for Reproducible Neuroimaging Computation is a vision
to help neuroscientists in reproducing neuroimaging research.

\subsection{Container Images Used}

\subsection{Image Update Process}

If there exist vulnerabilities in the container image, then there are two ways
to proceed, either not to run that image at all, or run only after updating the
packages that have vulnerabilities. The latter option effects the reproducibility
of the image, which means that the image cannot be used to verify research
findings of other scientists. An alternative approach can be to trim unnecessary
packages from images, which in turn reduces the number of vulnerabilities
present in images.


\subsection{Scanning tools used}

There are various scanning tools that are available for scanning container
images like Vuls, Clair, ClairScanner, Anchore, Dagda, etc. Out of them we
took Anchore and Vuls for our experiments to scan container
images which are used by the above discussed application frameworks.

\subsubsection{Anchore}

Anchore is an end-to-end container security, and open-source compliance plat-
form. Anchore is a static scanning tool that analyses container images and
lists operating system packages,
unofficial packages, configuration files, and language modules [58]. In other
words, Anchore is a set of tools, which users can employ for scanning container
images and applying custom policies to them. It provides visibility and trans-
parency of the container environment. Anchore offers services in two ways:
Anchore Engine and Anchore Engine Command Line Interface (CLI).
Anchore Engine is available as a Docker image, which is available on Docker
Hub. Anchore Engine CLI provides command line interface in addition to An-
chore Engine REST API. Anchore Engine CLI can be installed directly on the
host and used to inspect images.

\subsubsection{Vuls}

Vuls is an open-source vulnerability scanner for Linux/FreeBSD, written in Go
language. It is an agentless vulnerability scanner and uses various
vulnerability databases listed in 4.4a. Using dynamic scanning technique, it offers
remote and local scan. Remote scan involves setting up only one machine, to
which other target servers are connected via SSH. Local scan involves running
the scan on the target server itself. Further, Vuls offer fast scan, fast root scan,
and deep scan.
Fast scan is performed without root privileges, whereas fast root scan is
performed with root privileges. The latter also uses checkrestart utility to de-
tect processes that are updated but not yet restarted because if they are not
restarted, updates are not effective. So fast root scan also detects these pro-
cesses. Both fast scan and fast root scan are performed offline without Internet
access, and put almost no load on the scan target server. Deep scan uses
root privileges for scanning, and is used to get a more detailed scan. It checks
changelogs, which puts more load on the target server. As shown in Figure 4.3
it is clear that vulnerability detection time of Vuls is much better than An-
chore. Vuls tool uses dynamic scanning, so the scanning time does not depend
on the image size.
Graph 4.5 shows percentage of vulnerabilities that are
covered by Vuls and Anchore tools. This graph can be explained with the help
of List 4.4, which shows vulnerability databases that are referred, by both Vuls
and Anchore. Anchore covers less percentage of vulnerabilities as compared
to Vuls because Anchore relies mainly on vulnerability databases published by
Linux distributions, whereas Vuls additionally refers to another sources such as
databases from Japan, US-CERT, etc.

\subsection{CBRAIN}

Canadian Brain Imaging Research Platform (CBRAIN) is a web portal that is used
by CONP to exploit services of Compute Canada. CBRAIN is a collaborative
neuroimaging research platform that is active in production since 2009. CBRAIN
provides transparent access to various distributed storage sites and computing
resources across the world. CBRAIN’s infrastructure consists of three main
layers, namely access layer, service layer, and infrastructure layer. The access
layer is for CBRAIN users, and can be accessed through any modern browser or
RESTful API [42]. The service layer consists of CBRAIN portal, which contains
metadata and databases. It provides information about users, their permissions,
resources etc. The infrastructure layer consists of data resources and computing
resources, which are connected through a network.

\bibliographystyle{plain}
\bibliography{bibliography}


\end{document}

